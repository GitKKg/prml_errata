\documentclass[12pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage[margin=2cm]{geometry}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{url}

\newcommand{\erratum}[1]{\subsubsection*{#1}}

\begin{document}

\title{PRML Errata}
\author{\href{mailto:yousuketakada@gmail.com}{Yousuke Takada}}
\date{\today}
\maketitle

\section*{Preface}
This report communicates some possible errata for PRML~\citep{Bishop:PRML}
that are not listed in the official errata document~\citep{Svensen:PRML_errata}\footnote{\
The last line but one of the bibliographic information page of the copy of PRML I have
reads ``9 8 7 (corrected at 6th printing 2007).'' So I refer to Version~2 of the errata.}
at the time of this writing.
When specifying the location of an error, I follow the notational conventions adopted by
\citet{Svensen:PRML_errata}.
I have also included in this report some suggestions for improving the readability.

\section*{Corrections}

\erratum{Page~51}
Equation~(1.98):
Following the notation~(1.93), we should write the left hand side of (1.98) as
$\operatorname{H}[X]$ instead of $\operatorname{H}[p]$.

\erratum{Page~80}
Equation~(2.52):
We usually take eigenvectors~$\mathbf{u}_{i}$ to be the columns of $\mathbf{U}$ as in (C.37).
If we follow this convention, Equation~(2.52) and the following text should read
\begin{equation}
\mathbf{y} = \mathbf{U}^{\operatorname{T}} (\mathbf{x} - \bm{\mu})
\label{eq:change_of_variable_x_to_y}
\end{equation}
where $\mathbf{U}$ is a matrix whose columns are given by $\mathbf{u}_{i}$ so that
$\mathbf{U} = \left( \mathbf{u}_{1}, \dots,  \mathbf{u}_{D}\right)$.
From (2.46) it follows that $\mathbf{U}$ is an \emph{orthogonal} matrix, i.e.,
it satisfies $\mathbf{U}^{\operatorname{T}}\mathbf{U} = \mathbf{I}$ and hence also
$\mathbf{U}\mathbf{U}^{\operatorname{T}} = \mathbf{I}$ where $\mathbf{I}$ is the identity matrix.

\erratum{Page~81}
Equations~(2.53) and (2.54):
If we write the change of variable from $\mathbf{x}$ to $\mathbf{y}$ as
\eqref{eq:change_of_variable_x_to_y} instead of (2.52),
the Jacobian matrix~$\mathbf{J} = \left( J_{ij} \right)$ we require is
simply given by $\mathbf{U}$.
Equation~(2.53) should read
\begin{equation}
J_{ij} = \frac{\partial x_i}{\partial y_j} = U_{ij}
\end{equation}
where $U_{ij}$ is the $(i, j)$-th element of the matrix $\mathbf{U}$.
The square of the determinant of the Jacobian matrix~(2.54) can be evaluated as follows.
\begin{equation}
\left| \mathbf{J} \right|^{2} = \left| \mathbf{U} \right|^{2}
= \left| \mathbf{U}^{\operatorname{T}} \right| \left| \mathbf{U} \right|
= \left| \mathbf{U}^{\operatorname{T}} \mathbf{U} \right|
= \left| \mathbf{I} \right| = 1.
\end{equation}

\erratum{Page~81}
Line~$-1$:
Since the determinant of the Jacobian, which is an orthonormal matrix here, can be negative,
we should write $\left| \mathbf{J} \right| = \pm 1$
instead of $\left| \mathbf{J} \right| = 1$.

\erratum{Page~82}
Equation~(2.56):
We should take the absolute value of the determinant for the same reason given above;
the factor~$\left| \mathbf{J} \right|$ should read
$\left| \operatorname{det}\left( \mathbf{J} \right) \right|$.
Note that we cannot write $\left|\left| \mathbf{J} \right|\right|$ to mean
$\left| \operatorname{det}\left( \mathbf{J} \right) \right|$
because it is confusingly similar to the matrix norm~$\left\| \mathbf{J} \right\|$,
which usually refers to the largest singular value of $\mathbf{J}$ \citep{GvL:MC}.
This notational inconsistency has been caused by the abuse of the notation~$|\cdot|$ for
both the absolute value and the matrix determinant.
If we always use $\operatorname{det}(\cdot)$ for the determinant, confusion will not arise
and the notation be consistent.
An alternative solution to this problem would be to explicitly define
\begin{equation}
\left| \mathbf{A} \right| \equiv \left| \operatorname{det}\left( \mathbf{A} \right) \right|
\label{eq:abs_of_matrix_determinant}
\end{equation}
for any square matrix $\mathbf{A}$.
This notation is mostly consistent because we have
$\left| \mathbf{A} \right| = \operatorname{det}\left( \mathbf{A} \right)$ for a positive
semidefinite matrix $\mathbf{A}$ and
most other matrices for which we take determinants are positive (semi)definite in PRML.

\erratum{Page~102}
Equation~(2.155):
Although an interpretation for the parameters of the gamma distribution~(2.146) has been made,
one for the parameters of the Wishart distribution~(2.155) is not given here nor in Exercise~2.45.
In order to give one, let us consider a simple Bayesian inference problem in which,
given a set of $N$ observations~$\mathbf{X} =
\left\{ \mathbf{x}_1, \dots, \mathbf{x}_N \right\}$ for a zero-mean Gaussian random variable,
we infer the covariance matrix~$\bm{\Sigma}$ or, equivalently,
the precision matrix~$\bm{\Lambda} \equiv \bm{\Sigma}^{-1}$.
The likelihood~$p\left(\mathbf{X}\middle|\bm{\Lambda}\right)$
in terms of the precision~$\bm{\Lambda}$ is given by
\begin{equation}
p\left(\mathbf{X}\middle|\bm{\Lambda}\right) =
\prod_{n=1}^{N} p\left(\mathbf{x}_n\middle|\bm{\Lambda}\right) =
\prod_{n=1}^{N} \mathcal{N}\left(\mathbf{x}_n\middle|\mathbf{0}, \bm{\Lambda}^{-1}\right) .
\end{equation}
If we choose the prior~$p\left(\bm{\Lambda}\right)$ over $\bm{\Lambda}$ to be
a Wishart distribution so that
\begin{equation}
p\left(\bm{\Lambda}\right) = \mathcal{W}\left(\bm{\Lambda}\middle|\mathbf{W}_0, \nu_0\right)
\label{eq:wishart_prior}
\end{equation}
our analysis can be simplified because it is the conjugate prior.
In fact, the posterior~$p\left(\bm{\Lambda}\middle|\mathbf{X}\right)$ is given by
\begin{align}
p\left(\bm{\Lambda}\middle|\mathbf{X}\right) &\propto
p\left(\mathbf{X}\middle|\bm{\Lambda}\right) p\left(\bm{\Lambda}\right) \\
&\propto \left|\bm{\Lambda}\right|^{N/2}
\exp\left\{-\frac{1}{2}\sum_{n=1}^{N}\mathbf{x}_n^{\operatorname{T}}\bm{\Lambda}\mathbf{x}_n\right\}
\left|\bm{\Lambda}\right|^{(\nu_0 - D - 1)/2}
\exp\left\{-\frac{1}{2}\operatorname{Tr}\left(\mathbf{W}_0^{-1}\bm{\Lambda}\right)\right\} \\
&= \left|\bm{\Lambda}\right|^{(\nu_N - D - 1)/2}
\exp\left\{-\frac{1}{2}\operatorname{Tr}\left(\mathbf{W}_N^{-1}\bm{\Lambda}\right)\right\}
\end{align}
where
\begin{align}
\nu_N &= \nu_0 + N \\
\mathbf{W}_N^{-1} &=
\mathbf{W}_0^{-1} + \sum_{n=1}^{N}\mathbf{x}_n\mathbf{x}_n^{\operatorname{T}} .
\end{align}
Reinstating the normalization constant, we indeed see that the posterior becomes again
a Wishart distribution of the form
\begin{equation}
p\left(\bm{\Lambda}\middle|\mathbf{X}\right) =
\mathcal{W}\left(\bm{\Lambda}\middle|\mathbf{W}_N, \nu_N\right) .
\end{equation}
This result suggests us how we can interpret the parameters of
the Wishart distribution~(2.155), namely
the scale matrix~$\mathbf{W}$ and the number of degrees of freedom~$\nu$.
Since observing $N$ data points increases the number of degrees of freedom~$\nu$ by $N$,
we can interpret $\nu_0$ in the prior~\eqref{eq:wishart_prior} as
the number of ``effective'' prior observations.
The $N$ observations also contribute $N\bm{\Sigma}_{\text{ML}}$ to
the inverse of the scale matrix~$\mathbf{W}$
where $\bm{\Sigma}_{\text{ML}}$ is the maximum likelihood estimate for the covariance of
the observations given by
\begin{equation}
\bm{\Sigma}_{\text{ML}} = \frac{1}{N} \sum_{n=1}^{N}\mathbf{x}_n\mathbf{x}_n^{\operatorname{T}} .
\end{equation}
This suggests an interpretation of $\mathbf{W}$ in terms of
$\bm{\Sigma} \equiv \left(\nu\mathbf{W}\right)^{-1}$.
More specifically, we can interpret $\bm{\Sigma}_0 \equiv \left(\nu_0\mathbf{W}_0\right)^{-1}$ as
the covariance of the $\nu_0$ ``effective'' prior observations.
Note that this interpretation is in accordance with another observation that
the expectation of $\bm{\Lambda}$ taken over the prior~\eqref{eq:wishart_prior} is indeed given by
$\mathbb{E}\left[\bm{\Lambda}\right] = \nu_0\mathbf{W}_0 = \bm{\Sigma}_0^{-1}$
where we have used (B.80).

\erratum{Page~102}
Line~$-2$:
`Gamma' should read `gamma' (without capitalization).

\erratum{Page~104}
The text after Equation~(2.160):
The Gaussian~$\mathcal{N}\left(\mathbf{x}\middle|\bm{\mu}, \mathbf{\Lambda}\right)$
should read $\mathcal{N}\left(\mathbf{x}\middle|\bm{\mu}, \mathbf{\Lambda}^{-1}\right)$.

\erratum{Page~141}
Equation~(3.13):
The use of the gradient operator is not consistent here.
As in Equation~(2.224), the gradient (of a scalar function) is a column vector
so that Equation~(3.13) should read\footnote{\
I have not got the right typeface for the data vector
$\left( t_1, \dots, t_N \right)^{\operatorname{T}}$.
See ``Mathematical notation'' of PRML.}
\begin{equation}
\nabla \ln p \left( \mathbf{t} \middle| \mathbf{w}, \beta \right) =
\beta \sum_{n=1}^{N}
\left\{ t_n - \mathbf{w}^{\operatorname{T}} \bm{\phi}\left( \mathbf{x}_n \right) \right\}
\bm{\phi}\left( \mathbf{x}_n \right) .
\label{eq:gradient_of_linear_regression_likelihood}
\end{equation}
Moreover, I would like to also suggest that we should give a definition for the gradient
before we use it or, at least, in an appendix.
Although Appendix~C defines the vector derivative~$\frac{\partial}{\partial \mathbf{x}}$,
which is used interchangeably with the gradient~$\nabla_{\mathbf{x}}$ throughout PRML,
there is no mention of the gradient.
We shall come back to this issue later in this report.

\erratum{Page~142}
Equation~(3.14):
The left hand side should be a zero vector~$\mathbf{0}$ instead of a scalar zero~$0$.
Thus, Equation~(3.14) should read
\begin{equation}
\mathbf{0} =
\sum_{n=1}^{N} t_n \bm{\phi}\left( \mathbf{x}_n \right)
- \left( \sum_{n=1}^{N}
\bm{\phi}\left(\mathbf{x}_n\right)\bm{\phi}\left(\mathbf{x}_n\right)^{\operatorname{T}} \right)
\mathbf{w}
\end{equation}
where we have used the gradient of the form~\eqref{eq:gradient_of_linear_regression_likelihood}
instead of (3.13).

\erratum{Page~146}
Equation~(3.31):
The left hand side should be $\mathbf{y}\left(\mathbf{x}, \mathbf{W}\right)$ instead of
$\mathbf{y}\left(\mathbf{x}, \mathbf{w}\right)$.

\erratum{Page~166}
The second paragraph, Line~1:
`Gamma' should read `gamma' (without capitalization).

\erratum{Pages~168--169, and 177}
Equations~(3.88), (3.93), and (3.117) as well as the text before (3.93):
The derivative operators should be partial differentials.
For example, Equation~(3.117) should read
\begin{equation}
\frac{\partial}{\partial \alpha} \ln \left| \mathbf{A} \right| =
\operatorname{Tr}\left( \mathbf{A}^{-1} \frac{\partial}{\partial \alpha} \mathbf{A} \right) .
\end{equation}

\erratum{Page~207}
Equation~(4.92):
On the right hand side, the gradient and the Hessian,
which are in general functions of the parameter~$\mathbf{w}$,
must be evaluated at the previous estimate~$\mathbf{w}^{\text{old}}$ for the parameter.
Thus, Equation~(4.92) should read
\begin{equation}
\mathbf{w}^{\text{new}} = \mathbf{w}^{\text{old}} -
  \left[ \mathbf{H} \left( \mathbf{w}^{\text{old}} \right) \right]^{-1}
  \nabla E \left( \mathbf{w}^{\text{old}} \right)
\end{equation}
where $\mathbf{H}\left(\mathbf{w}\right) \equiv \nabla\nabla E\left(\mathbf{w}\right)$ is
the Hessian matrix whose elements comprise the second derivatives of $E\left(\mathbf{w}\right)$
with respect to the components of $\mathbf{w}$.

\erratum{Page~210}
Equation~(4.110):
The left hand side of (4.110),
which is obtained by taking the gradient of $\nabla_{\mathbf{w}_{j}} E$ given in (4.109)
with respect to $\mathbf{w}_{k}$,
refers to the $(k, j)$-th block of the Hessian, \emph{not} the $(j, k)$-th.
Thus, Equation~(4.110) should read
\begin{equation}
\nabla_{\mathbf{w}_{k}} \nabla_{\mathbf{w}_{j}}
E \left( \mathbf{w}_{1}, \dots, \mathbf{w}_{K} \right)
  = \sum_{n=1}^{N} y_{nj} \left( I_{kj} - y_{nk} \right)
    \bm{\phi}_{n} \bm{\phi}_{n}^{\operatorname{T}}.
\end{equation}
To be clear, we have used the following notation.
If we group all the parameters~$\mathbf{w}_{1}$, $\dots$, $\mathbf{w}_{K}$ into
a column vector
\begin{equation}
\mathbf{w} =
  \begin{pmatrix}
  \mathbf{w}_{1} \\
  \vdots \\
  \mathbf{w}_{K}
  \end{pmatrix}
\end{equation}
the gradient and the Hessian of the error function~$E\left(\mathbf{w}\right)$ with respect to
$\mathbf{w}$ are given by
\begin{equation}
\nabla_{\mathbf{w}} E =
  \begin{pmatrix}
  \nabla_{\mathbf{w}_{1}} E \\
  \vdots \\
  \nabla_{\mathbf{w}_{K}} E
  \end{pmatrix} , \qquad
\nabla_{\mathbf{w}} \nabla_{\mathbf{w}} E =
  \begin{pmatrix}
  \nabla_{\mathbf{w}_{1}} \nabla_{\mathbf{w}_{1}} E &
  \cdots &
  \nabla_{\mathbf{w}_{1}} \nabla_{\mathbf{w}_{K}} E \\
  \vdots & \ddots & \vdots \\
  \nabla_{\mathbf{w}_{K}} \nabla_{\mathbf{w}_{1}} E &
  \cdots &
  \nabla_{\mathbf{w}_{K}} \nabla_{\mathbf{w}_{K}} E
  \end{pmatrix}
\end{equation}
respectively.

\erratum{Page~239}
Figure~5.6:
The eigenvectors~$\mathbf{u}_{1}$ and $\mathbf{u}_{2}$ in Figure~5.6 are unit vectors;
their orientations should be shown as in Figure~2.7.
Or, the scaled vectors should be labeled as
$\lambda_{1}^{-1/2}\mathbf{u}_{1}$ and $\lambda_{2}^{-1/2}\mathbf{u}_{2}$.

\erratum{Page~251}
The second paragraph:
The approximation of the form~(5.84) is usually referred to as
the \emph{Gauss--Newton} approximation, but not \emph{Levenberg--Marquardt}.
The Levenberg--Marquardt method is a method that improves
the numerical stability of (Gauss--)Newton iterations
by correcting the Hessian matrix so as to be more diagonal dominant~\citep{Press:NR}.

\erratum{Page~275}
The text after Equation~(5.154):
The identity matrix~$\mathbf{I}$ should multiply $\sigma_{k}^{2}(\mathbf{x}_{n})$.

\erratum{Page~277}
Equation~(5.160):
The factor~$L$ should multiply $\sigma_{k}^{2}(\mathbf{x})$ because we have
\begin{align}
s^2(\mathbf{x}) &=
  \mathbb{E}\left[
    \operatorname{Tr}\left\{
      \left( \mathbf{t} - \mathbb{E}\left[ \mathbf{t} \middle| \mathbf{x} \right] \right)
      \left( \mathbf{t} - \mathbb{E}\left[ \mathbf{t} \middle| \mathbf{x} \right]
        \right)^{\operatorname{T}} \right\}
  \middle| \mathbf{x} \right] \\
&= \sum_{k=1}^{K} \pi_{k}(\mathbf{x})
  \operatorname{Tr}\left\{ \sigma_{k}^{2}(\mathbf{x})\mathbf{I} +
    \left( \bm{\mu}_{k}(\mathbf{x}) -
      \mathbb{E}\left[ \mathbf{t} \middle| \mathbf{x} \right] \right)
    \left( \bm{\mu}_{k}(\mathbf{x}) -
      \mathbb{E}\left[ \mathbf{t} \middle| \mathbf{x} \right] \right)^{\operatorname{T}}
  \right\} \\
&= \sum_{k=1}^{K} \pi_{k}(\mathbf{x})
  \left\{ L \sigma_{k}^{2}(\mathbf{x}) +
    \left\| \bm{\mu}_{k}(\mathbf{x}) -
      \mathbb{E}\left[ \mathbf{t} \middle| \mathbf{x} \right] \right\|^{2}
  \right\}
\end{align}
where $L$ is the dimensionality of $\mathbf{t}$.

\erratum{Page~295}
Line~1:
The vector~$\mathbf{x}$ should be a column vector so that
$\mathbf{x} = \left( x_1, x_2 \right)^{\operatorname{T}}$.

\erratum{Page~318}
The text before Equation~(6.93) as well as Equations~(6.93) and (6.94):
The text and the equations should read:
We can evaluate the derivative of $a_n^{\star}$ with respect to $\theta_j$ by differentiating
the relation~(6.84) with respect to $\theta_j$ to give
\begin{equation}
\frac{\partial \mathbf{a}_N^{\star}}{\partial \theta_j}
 = \frac{\partial\mathbf{C}_N}{\partial \theta_j} \left( \mathbf{t}_N - \bm{\sigma}_N \right)
   - \mathbf{C}_N \mathbf{W}_N \frac{\partial \mathbf{a}_N^{\star}}{\partial \theta_j}
   \label{eq:jacobian_of_a_N_wrt_theta_j}
\end{equation}
where the derivatives are Jacobians defined by (C.16) for a vector and
analogously by \eqref{eq:Jacobian_of_matrix_wrt_scalar} for a matrix.
Rearranging \eqref{eq:jacobian_of_a_N_wrt_theta_j} then gives
\begin{equation}
\frac{\partial \mathbf{a}_N^{\star}}{\partial \theta_j}
 = \left( \mathbf{I} + \mathbf{C}_N \mathbf{W}_N \right)^{-1}
   \frac{\partial\mathbf{C}_N}{\partial \theta_j} \left( \mathbf{t}_N - \bm{\sigma}_N \right) .
\end{equation}

\erratum{Page~355}
Equation~(7.117):
The typeface of the vector~$\mathbf{y}$ in (7.117) should be that in (7.110).

\erratum{Page~414}
Figure~8.53, Line~6:
The term~``max-product'' should be ``max-sum.''

\erratum{Page~425}
Equation~(9.3):
The right hand side should be a zero vector~$\mathbf{0}$ instead of a scalar zero~$0$.

\erratum{Page~432}
The text after Equation~(9.13):
It should be noted for clarity that, as the prior~$p(\mathbf{z})$ over $\mathbf{z}$ is
a multinomial distribution~(9.10),
the posterior~$p \left( \mathbf{z} \middle| \mathbf{x} \right)$ over $\mathbf{z}$
given $\mathbf{x}$ is again a multinomial of the form
\begin{equation}
p \left( \mathbf{z} \middle| \mathbf{x} \right) = \prod_{k=1}^{K} \gamma_k^{z_k}
\end{equation}
where we have written $\gamma_k \equiv \gamma(z_k)$,
which can be directly confirmed by inspecting the functional form of the joint distribution
\begin{equation}
p \left( \mathbf{z} \right) p \left( \mathbf{x} \middle| \mathbf{z} \right) =
 \prod_{k=1}^{K} \left\{ \pi_k \,
 \mathcal{N} \left( \mathbf{x} \middle| \bm{\mu}_k, \bm{\Sigma}_k \right) \right\}^{z_k} .
\end{equation}
This observation helps the reader to understand that
evaluating the responsibilities~$\gamma(z_k)$ indeed corresponds to
the E~step of the general EM~algorithm.

\erratum{Page~434}
Equation~(9.15):
Although the official errata~\citep{Svensen:PRML_errata} states that
$\sigma_j$ on the right hand side should be raised to a power of $D$,
the whole right hand side should be raised to $D$ so that Equation~(9.15) should read
\begin{equation}
\mathcal{N}\left( \mathbf{x}_n \middle| \mathbf{x}_n, \sigma_j^2 \mathbf{I} \right)
 = \frac{1}{\left( 2 \pi \sigma_j^2 \right)^{D/2}} .
\end{equation}

\erratum{Page~435}
Equation~(9.16):
The right hand side should be a zero vector~$\mathbf{0}$.

\erratum{Page~465}
Equations~(10.6) and (10.7):
In PRML, Equation (10.6) will be later recognized as ``a negative Kullback-Leibler divergence
between $q_j(\mathbf{Z}_j)$ and $\tilde{p}(\mathbf{X}, \mathbf{Z}_j)$''
(Page~465, Line~$-2$).
However, there is no point in taking a Kullback-Leibler divergence between two probability
distributions over different sets of random variables; such a quantity is undefined.
Moreover, the discussion here seems to be somewhat redundant.
We actually do not have to introduce
the probability~$\tilde{p}(\mathbf{X}, \mathbf{Z}_j)$ other than $q_j^{\star}(\mathbf{Z}_j)$.
Specifically, we can rewrite Equations~(10.6) and (10.7) into
\begin{align}
\mathcal{L}(q) &= \dots \\
&= \int q_j \ln q_j^{\star} \mathrm{d}\mathbf{Z}_j - \int q_j \ln q_j \mathrm{d}\mathbf{Z}_j
 + \text{const} \\
&= - \operatorname{KL}\left( q_j \middle\| q_j^{\star} \right) + \text{const}
\label{eq:variational_optimization_of_lower_bound_wrt_q_j}
\end{align}
where we have defined a new distribution $q_j^{\star}\left(\mathbf{Z}_j\right)$ by the relation
\begin{equation}
\ln q_j^{\star}\left(\mathbf{Z}_j\right) =
\mathbb{E}_{i \neq j}\left[ \ln p\left(\mathbf{X}, \mathbf{Z}\right) \right] + \text{const}.
\label{eq:optimal_solution_for_variational_bayes}
\end{equation}
It directly follows from \eqref{eq:variational_optimization_of_lower_bound_wrt_q_j} that,
since the lower bound $\mathcal{L}(q)$ is the negative Kullback-Leibler divergence between
$q_j(\mathbf{Z}_j)$ and $q_j^{\star}(\mathbf{Z}_j)$ up to some additive constant,
the maximum of $\mathcal{L}(q)$ occurs when $q_j(\mathbf{Z}_j) = q_j^{\star}(\mathbf{Z}_j)$.

\erratum{Page~465}
The text before Equation~(10.8):
The latent variable~$\mathbf{z}_i$ should read $\mathbf{Z}_i$.

\erratum{Page~465}
Line~$-1$:
If we adopt the representation~\eqref{eq:variational_optimization_of_lower_bound_wrt_q_j},
the probability~$\tilde{p}(\mathbf{X}, \mathbf{Z}_j)$ should read
$q_j^{\star}\left(\mathbf{Z}_j\right)$.

\erratum{Page~466}
Line~1:
Again, $\tilde{p}(\mathbf{X}, \mathbf{Z}_j)$ should read
$q_j^{\star}\left(\mathbf{Z}_j\right)$.
The sentence~``Thus we obtain...'' should read
``Thus we see that we have already obtained a general expression for the optimal solution in
\eqref{eq:optimal_solution_for_variational_bayes}.''

\erratum{Page~468}
The text after Equation~(10.16):
The constant term in (10.16) is the \emph{negative} entropy of $p(\mathbf{Z})$.

\erratum{Page~478}
Equation~(10.63):
The additive constant $+1$ on the right hand side should be omitted so that
Equation~(10.63) should read
\begin{equation}
\nu_k = \nu_0 + N_k . \label{eq:reestimation_equation_for_nu}
\end{equation}
A quick check for the correctness of the re-estimation equations would be to consider
a limit of $N \to 0$, in which the effective number of observations~$N_k$ also goes to
zero and the re-estimation equations should reduce to identities.
Equation~(10.63) does not reduces to $\nu_k = \nu_0$, failing the test.
Note that the solution for Exercise~10.13 given by \citet{Svensen:PRML_web_solution} correctly
derives the result~\eqref{eq:reestimation_equation_for_nu}.

\erratum{Page~489}
Equation~(10.107):
The expectations~$\mathbb{E}_{\alpha}\left[ \ln q(\mathbf{w}) \right]_{\mathbf{w}}$ and
$\mathbb{E}\left[ \ln q(\alpha) \right]$ should read
$\mathbb{E}_{\mathbf{w}}\left[ \ln q(\mathbf{w}) \right]$ and
$\mathbb{E}_{\alpha}\left[ \ln q(\alpha) \right]$, respectively,
where the expectation~$\mathbb{E}_{\mathbf{Z}}[\cdot]$ is taken over $q\left(\mathbf{Z}\right)$.

\erratum{Page~489}
Equations~(10.108) through (10.112):
The expectations are notationally inconsistent with (1.36);
they should be of the forms shown in (10.107) or the ones corrected as above.

\erratum{Page~490}
The third paragraph, Line~2:
A comma (,) should be inserted after the ellipsis so that the range of $n$ should read:
$n = 1, \dots, N$.

\erratum{Page~496}
Equation~(10.140):
In order to be consistent with the mathematical notations in PRML,
the differential operator~$d$ in (10.140) should be upright~$\mathrm{d}$.
Moreover, the derivative of $x$ with respect to $x^2$ should be written with parentheses as
$\frac{\mathrm{d}x}{\mathrm{d}\left(x^2\right)}$,
instead of $\frac{\mathrm{d}x}{\mathrm{d}x^2}$, to avoid ambiguity.

\erratum{Page~501}
The text after Equation~(10.162):
The variational parameter $\lambda(\xi)$ is a monotonic function of $\xi$ for $\xi \ge 0$,
but not that its derivative~$\lambda'(\xi)$ is.

\erratum{Page~503}
The text after Equation~(10.168):
A period (.) should be added at the end of the sentence that follows (10.168).

\erratum{Page~512}
Equation~(10.222):
The factor~$\left( 2 \pi v_n \right)^{D/2}$ in the denominator of the right hand side should be
omitted because it has been already included in the Gaussian in (10.213).

\erratum{Page~513}
Equations~(10.223) and (10.224):
The quantities $v^{\text{new}}$ and $\mathbf{m}^{\text{new}}$ in (10.223) and (10.224) are
different from those in (10.217) and (10.218).\footnote{See \citet{Svensen:PRML_errata}
for the errata for Equations~(10.217) and (10.218).}
Thus, we should introduce different notations, say, $v$ and $\mathbf{m}$, with appropriate
definitions.
Specifically, one can rewrite the approximation to the model evidence in the form
\begin{equation}
p(\mathcal{D}) \simeq \left( 2\pi v \right)^{D/2} \exp\left( B/2 \right)
  \prod_{n=1}^{N} \left\{ s_n \left( 2\pi v_n \right)^{-D/2} \right\}
\end{equation}
where
\begin{align}
B &= \frac{\mathbf{m}^{\operatorname{T}}\mathbf{m}}{v}
  - \sum_{n=1}^{N} \frac{\mathbf{m}_n^{\operatorname{T}}\mathbf{m}_n}{v_n} \\
v^{-1} &= \sum_{n=1}^{N} v_n^{-1} \\
v^{-1} \mathbf{m} &= \sum_{n=1}^{N} v_n^{-1} \mathbf{m}_n .
\end{align}

\erratum{Page~515}
Equations~(10.228) and (10.229):
Although \citet{Svensen:PRML_errata} correct (10.228) so that $q^{\backslash b}(\mathbf{x})$ is
a normalized distribution,
we do not need the normalization of $q^{\backslash b}(\mathbf{x})$ here and,
even with this normalization, we cannot ensure that $\hat{p}(\mathbf{x})$ given by (10.229)
is normalized.
Similarly to (10.195), we can proceed with the unnormalized $q^{\backslash b}(\mathbf{x})$ given by
the original (10.228) and, rather than correcting (10.228), we should correct (10.229) so that
\begin{equation}
\hat{p}(\mathbf{x}) \propto q^{\backslash b}(\mathbf{x}) f_b(x_2, x_3) = \dots
\end{equation}
implying that $\hat{p}(\mathbf{x})$ is a normalized distribution.

\erratum{Page~515}
The text after Equation~(10.229):
The new distribution~$q^{\text{new}}(\mathbf{z})$ should read $q^{\text{new}}(\mathbf{x})$.

\erratum{Page~516}
Equation~(10.240):
The subscript~$k$ of the product~$\displaystyle\prod_{k} \dots$ should read $k \neq j$
because we have already removed the term~$\tilde{f}_j(\bm{\theta}_j)$.

\erratum{Page~554}
Equation~(11.72), Line~$-2$:
If we write the expectation $\mathbb{E}_{\mathbf{z}}[\cdot]$ taken over
some given distribution $q(\mathbf{z})$ explicitly as
$\mathbb{E}_{q(\mathbf{z})}\left[\cdot\right]$,
the expectation in the last line but one of (11.72) should read
\begin{equation}
\mathbb{E}_{p_{G}(\mathbf{z})} \left[ \exp\left(-E(\mathbf{z}) + G(\mathbf{z}) \right) \right]
\end{equation}
where we have written the argument~$\mathbf{z}$ for $E(\mathbf{z})$ and $G(\mathbf{z})$
for clarity.

\erratum{Page~556}
Exercise~11.7:
The interval should be $\left[ -\pi/2 , \pi/2 \right]$ instead of $[0, 1]$.

\erratum{Page~557}
Exercise~11.14, Line~2:
The variance should be $\sigma_i^2$ instead of $\sigma_i$.

\erratum{Page~564}
The text after Equation~(12.12):
The derivative we consider here is that with respect to $b_j$
(\emph{not} that with respect to $b_i$).

\erratum{Page~564}
The text after Equation~(12.15):
The zero should be a zero vector so that we have $\mathbf{u}_j = \mathbf{0}$.

\erratum{Page~575}
The third paragraph, Line~5:
The zero vector should be a row vector instead of a column vector so that we have
$\mathrm{v}^{\operatorname{T}}\mathbf{U} = \mathbf{0}^{\operatorname{T}}$.
Or, the both sides are transposed to give $\mathbf{U}^{\operatorname{T}}\mathbf{v} = \mathbf{0}$.

\erratum{Page~578}
Equation~(12.53):
As stated in the text preceding (12.53),
we should substitute $\bm{\mu} = \bar{\mathbf{x}}$ into (12.53).

\erratum{Page~578}
The text before Equation~(12.56):
For the maximization with respect to $\mathbf{W}$, we use (C.25) and (C.27) instead of (C.24).

\erratum{Page~579}
Line~5:
The eigendecomposition requires $O(D^3)$ computation\emph{s}.

\erratum{Page~599}
Exercise~12.1, Line~$-1$:
The quantity $\lambda_{M + 1}$ is an eigenvalue (not an eigenvector).

\erratum{Page~602}
Exercise~12.25, Line~2:
The latent space distribution should read
$p(\mathbf{z}) = \mathcal{N}\left( \mathbf{z} \middle| \mathbf{0}, \mathbf{I}\right)$.

\erratum{Page~610}
The first paragraph, Line~$-5$:
The text~``our predictions ...'' should read: ``our predictions for $\mathbf{x}_{n + 1}$
depend on all the previous observations.''

\erratum{Page~620}
The second paragraph and the following (unlabeled) equation:
The last sentence before the equation and the equation each should terminate with a period (.).

\erratum{Page~621}
Figures~13.12 and 13.13:
It should be clarified that, similarly to $\alpha(z_{nk})$ and $\beta(z_{nk})$,
the notation~$p\left(\mathbf{x}_n \middle| z_{nk}\right)$ is used to denote
the value of $p\left(\mathbf{x}_n \middle| \mathbf{z}_{n}\right)$ when $z_{nk} = 1$.

\erratum{Page~622}
The second paragraph, Line~$-1$:
``we see'' should be omitted.

\erratum{Page~623}
The first paragraph, Line~$-2$:
$z_{nk}$ should read $z_{n - 1, k}$.

\erratum{Page~631}
Equation~(13.73):
The equation should read
\begin{equation}
\sum_{r=1}^{R} \ln \left\{
  \frac{p\left(\mathbf{X}_{r}\middle|\bm{\theta}_{m_r}\right)p(m_r)}{
    \sum_{l=1}^{M}p\left(\mathbf{X}_{r}\middle|\bm{\theta}_{l}\right)p(l)} \right\} .
\end{equation}

\erratum{Page~637}
Equations~(13.81), (13.82), and (13.83):
The distribution~(13.81) over $\mathbf{w}$ should read
\begin{equation}
p(\mathbf{w}) = \mathcal{N}\left(\mathbf{w}\middle|\mathbf{0}, \bm{\Gamma}\right)
\end{equation}
and so on.

\erratum{Page~638}
The first paragraph, Line~2:
``conditional on'' should read ``conditioned on.''

\erratum{Page~641}
The text after Equation~(13.103):
The form of the Gaussian is unclear.
Since a multivariate Gaussian is usually defined over a column vector,
we should construct a column vector from the concerned random variables to clearly define the mean
and the covariance.
Specifically, the text should read for example:
\dots, we see that $\xi(\mathbf{z}_{n-1}, \mathbf{z}_{n})$ is a Gaussian of the form
\begin{equation}
\xi(\mathbf{z}_{n-1}, \mathbf{z}_{n}) =
  \mathcal{N}\left(
    \begin{pmatrix}
      \mathbf{z}_{n-1} \\
      \mathbf{z}_{n}
    \end{pmatrix}
  \middle|
    \begin{pmatrix}
    \hat{\bm{\mu}}_{n-1} \\
    \hat{\bm{\mu}}_{n}
    \end{pmatrix},
    \begin{pmatrix}
      \hat{\mathbf{V}}_{n-1} & \hat{\mathbf{V}}_{n-1, n} \\
      \hat{\mathbf{V}}_{n-1, n}^{\operatorname{T}} & \hat{\mathbf{V}}_{n}
    \end{pmatrix}
  \right)
\end{equation}
where the mean $\hat{\bm{\mu}}_{n}$ and the covariance $\hat{\mathbf{V}}_{n}$ of
$\mathbf{z}_{n}$ are given by (13.100) and (13.101), respectively;
and the covariance $\hat{\mathbf{V}}_{n-1, n}$ between $\mathbf{z}_{n-1}$ and $\mathbf{z}_{n}$ 
is given by
\begin{equation}
\hat{\mathbf{V}}_{n-1, n}
  = \operatorname{cov}\left[\mathbf{z}_{n-1}, \mathbf{z}_{n}\right]
  = \mathbf{J}_{n-1}\hat{\mathbf{V}}_{n} .
\end{equation}

\erratum{Pages~642 and 643}
Equation~(13.109) and the following equations:
If we follow the notation in Chapter~9, the typeface of the $Q$ function should be $\mathcal{Q}$.

\erratum{Page~642}
Equation~(13.109):
If we follow the notation for a conditional expectation given by (1.37),
Equation~(13.109) should read
\begin{align}
\mathcal{Q}\left(\bm{\theta}, \bm{\theta}^{\text{old}}\right)
  &= \mathbb{E}_{\mathbf{Z}} \left[
       \ln p \left( \mathbf{X}, \mathbf{Z} \middle| \bm{\theta} \right) \middle|
       \mathbf{X}, \bm{\theta}^{\text{old}} \right] \\
  &= \int \! \mathrm{d}\mathbf{Z} \;
       p \left( \mathbf{Z} \middle| \mathbf{X}, \bm{\theta}^{\text{old}}\right)
       \ln p \left( \mathbf{X}, \mathbf{Z} \middle| \bm{\theta} \right)
\end{align}
which corresponds to (9.30).

\erratum{Page~643}
Equation~(13.111):
$\mathbf{V}_{0}^{\text{new}}$ should read $\mathbf{P}_{0}^{\text{new}}$.
\citet{Svensen:PRML_errata} have failed to mention (13.111).

\erratum{Page~643}
Equation~(13.114): The size of the opening curly brace `$\{$' should match
that of the closing curly brace `$\}$'.

\erratum{Page~647}
Figure~13.23, Line $-1$:
$p(\mathbf{x}_{n+1}|\mathbf{z}_{n+1}^{(l)})$ should read $p(\mathbf{x}_{n+1}|{z}_{n+1}^{(l)})$.

\erratum{Page~649}
Exercise~13.14, Line 1: (8.67) should be (8.64).

\erratum{Page~658}
Figure~14.1, the equation below:
The subscript of the summation in the right hand side should read $m = 1$.

\erratum{Page~668}
Equation~(14.37):
The arguments of the probability are notationally inconsistent with those of (14.34), (14.35),
and (14.36).
Specifically, the conditioning on $\bm{\phi}_n$ should read that on $t_n$ and
the probability~$p(k|\dots)$ be the value of $p(\mathbf{z}_{n}|\dots)$ when $z_{nk} = 1$,
which we write $p(z_{nk} = 1|\dots)$.
Moreover, strictly speaking,
the old parameters~$\pi_k, \mathbf{w}_k, \beta$ should read
$\pi_k^{\text{old}}, \mathbf{w}_k^{\text{old}}, \beta^{\text{old}} \in \bm{\theta}^{\text{old}}$.
In order to solve these problems, we should rewrite Equation~(14.37) as, for example,
\begin{equation}
\gamma_{nk} = \mathbb{E}\left[ z_{nk} \middle| t_n, \bm{\theta}^{\text{old}} \right]
\end{equation}
where we have written the conditioning in the expectation explicitly and
the expectation is given by
\begin{equation}
\mathbb{E}\left[ z_{nk} \middle| t_n, \bm{\theta} \right]
  = p\left( z_{nk} = 1 \middle| t_n, \bm{\theta} \right)
  = \frac{\pi_k \,
      \mathcal{N}\left(t_n\middle|\mathbf{w}_k^{\operatorname{T}}\bm{\phi}_n, \beta^{-1}\right)}{
    \sum_j \pi_j \,
      \mathcal{N}\left(t_n\middle|\mathbf{w}_j^{\operatorname{T}}\bm{\phi}_n, \beta^{-1}\right)}.
\end{equation}

\erratum{Page~668}
The unlabeled equation between (14.37) and (14.38):
If we write the implicit conditioning in the expectation explicitly
(similarly to the above equations), the unlabeled equation should read
\begin{align}
\mathcal{Q}\left(\bm{\theta}, \bm{\theta}^{\text{old}}\right)
  &= \mathbb{E}_{\mathbf{Z}}\left[ \ln p(\mathbf{t}, \mathbf{Z}|\bm{\theta}) \middle|
       \mathbf{t}, \bm{\theta}^{\text{old}} \right] \\
  &= ...
\end{align}
where we have again used the typeface~$\mathcal{Q}$ for the $Q$ function so that the notation
is consistent with that of Chapter~9.

\erratum{Page~669}
Equations~(14.40) and (14.41): The left hand sides should read a zero vector~$\mathbf{0}$.

\erratum{Page~669}
Equation~(14.41): $\bm{\Phi}$ is undefined.
The text following (14.41) should read:
\dots where $\mathbf{R}_k = \operatorname{diag}(\gamma_{nk})$ is
a diagonal matrix of size $N \times N$ and
$\bm{\Phi} = \left( \bm{\phi}_1, \dots, \bm{\phi}_N \right)^{\operatorname{T}}$ is
an $N \times M$ matrix.
Here, $N$ is the size of the data set and
$M$ is the dimensionality of the feature vectors $\bm{\phi}_n$.

\erratum{Page~669}
Equation~(14.43): `$+\text{const}$' should be added to the right hand side.

\erratum{Page~671}
The text after Equation~(14.46):
The text should read: \dots where we have omitted the dependence on $\{\bm{\phi}_n\}$ and
defined $y_{nk} = \dots$.
Or, $\bm{\phi}$ should have been omitted from the left hand side of (14.45).

\erratum{Page~671}
Equation~(14.48):
The notation should be corrected similarly to the above erratum regarding (14.37).

\erratum{Page~671}
Equation~(14.49):
The notation should be corrected similarly to the above erratum regarding the unlabeled equation
between (14.37) and (14.38).

\erratum{Page~672}
Equation~(14.52):
The negation should be removed so that $\mathbf{H}_k \equiv \nabla_k \nabla_k \mathcal{Q}$ where
\begin{equation}
\nabla_k \nabla_k \mathcal{Q}
   = - \sum_{n=1}^{N} \gamma_{nk} y_{nk} (1 - y_{nk}) \bm{\phi}_n \bm{\phi}_n^{\operatorname{T}}.
\end{equation}

\erratum{Page~674}
Exercise~14.1, Line~1:
``of'' should be inserted after ``set.''

\erratum{Page~686}
Line~$-3$:
The comma in the first inline math should be removed so that the product should read:
$m \times (m-1) \times \dots \times 2 \times 1$.

\erratum{Page~687}
Equation~(B.25):
The differential operator~$d$ should be upright~$\mathrm{d}$.

\erratum{Page~688}
Line~1:
`Gamma' should read `gamma' (without capitalization).

\erratum{Page~689}
Line~1: `positive-definite' should read `positive definite' (without hyphenation).

\erratum{Page~689}
Equation~(B.49):
$\mathbf{x}$ in the right hand side should read $\mathbf{x}_{a}$.

\erratum{Page~690}
Equation~(B.54):
When identifying the functional form of this distribution in,
e.g., the posterior distribution for the Gaussian mixture model of Section~9.2,
I have found it helpful to write the distribution in terms of unnormalized probabilities
$\tilde{\mu}_k \geq 0$ so that
\begin{equation}
\operatorname{Mult}\left(\mathbf{x}\middle|\tilde{\bm{\mu}}\right) =
  C(\tilde{\bm{\mu}}) \prod_{k=1}^{K} {\tilde{\mu}_k}^{x_k}
\end{equation}
where we have defined the normalization constant as
\begin{equation}
{C(\tilde{\bm{\mu}})}^{-1} = \sum_{k=1}^{K} \tilde{\mu}_k .
\end{equation}
and, thus, we have $\mu_k = C(\tilde{\bm{\mu}}) \tilde{\mu}_k$.

\erratum{Page~692}
Equation~(B.68):
This form of multivariate Student's t-distribution is derived in Section~2.3.7
by marginalizing over a gamma distributed scalar variable $\eta$ in (2.161),
but \emph{not} by marginalizing over the precision matrix that is generated from
the conjugate Wishart distribution, which results in a distribution of the form
\begin{equation}
p\left(\mathbf{x}\middle|\bm{\mu}, \mathbf{W}, \nu\right) =
  \int \mathcal{N}\left(\mathbf{x}\middle|\bm{\mu}, \bm{\Lambda}^{-1}\right)
    \mathcal{W}\left(\bm{\Lambda}\middle|\mathbf{W}, \nu\right) \mathrm{d}\bm{\Lambda} .
\label{eq:multivariate_t_marginalized_over_precision}
\end{equation}
I am not sure how we can marginalize over the precision matrix and, moreover,
whether the marginalized distribution \eqref{eq:multivariate_t_marginalized_over_precision} is
equivalent to (B.68).
Some appropriate citation is needed if such details are omitted from the text.

\erratum{Page~693}
Equations~(B.78) through (B.82):
Some appropriate citation is needed for the Wishart distribution
because it has been introduced in Section~2.3.6 without any proof for
the normalization constant as well as other statistics.

\erratum{Page~693}
Line~$-1$: $b = 1/2W$ should read $b = 1/(2W)$ for clarity.

\erratum{Page~696}
Equation~(C.5):
Replacing $\mathbf{B}^{\operatorname{T}}$ with $\mathbf{A}$, we obtain a more general identity
\begin{equation}
\left( \mathbf{P}^{-1} + \mathbf{A}\mathbf{R}^{-1}\mathbf{B} \right)^{-1}
\mathbf{A}\mathbf{R}^{-1}
=
\mathbf{P}\mathbf{A}
\left( \mathbf{B}\mathbf{P}\mathbf{A} + \mathbf{R} \right)^{-1}
\label{eq:general_push_through_identity}
\end{equation}
which is necessary to show the \emph{push-through identity}~(C.6) and also
the determinant identity~(C.14).
As suggested in the text,
the above identity~\eqref{eq:general_push_through_identity} can be
directly verified by right multiplying both sides by
$\left( \mathbf{B}\mathbf{P}\mathbf{A} + \mathbf{R} \right)$.
However, I would prefer to prove
the general push-through identity~\eqref{eq:general_push_through_identity}
together with the Woodbury identity~(C.7) in terms of the inverse of a partitioned matrix,
which we have already seen in Section~2.3.1.
To this end, we first introduce a square matrix $\mathbf{M}$ that is partitioned into
four submatrices so that
\begin{equation}
\mathbf{M} =
\begin{pmatrix}
\mathbf{A} & \mathbf{B} \\
\mathbf{C} & \mathbf{D}
\end{pmatrix}
\end{equation}
where $\mathbf{A}$ and $\mathbf{D}$ are square (but not necessarily the same dimension)
and then note that $\mathbf{M}$ can be block diagonalized as
\begin{equation}
\begin{pmatrix}
\mathbf{I} & \mathbf{O} \\
-\mathbf{C}\mathbf{A}^{-1} & \mathbf{I}
\end{pmatrix}
\begin{pmatrix}
\mathbf{A} & \mathbf{B} \\
\mathbf{C} & \mathbf{D}
\end{pmatrix}
\begin{pmatrix}
\mathbf{I} & -\mathbf{A}^{-1}\mathbf{B} \\
\mathbf{O} & \mathbf{I}
\end{pmatrix}
=
\begin{pmatrix}
\mathbf{A} & \mathbf{O} \\
\mathbf{O} & \mathbf{M}/\mathbf{A}
\end{pmatrix}
\label{eq:block_diagonalization_of_partitioned_matrix_with_Schur_complement_wrt_A}
\end{equation}
or
\begin{equation}
\begin{pmatrix}
\mathbf{I} & -\mathbf{B}\mathbf{D}^{-1} \\
\mathbf{O} & \mathbf{I}
\end{pmatrix}
\begin{pmatrix}
\mathbf{A} & \mathbf{B} \\
\mathbf{C} & \mathbf{D}
\end{pmatrix}
\begin{pmatrix}
\mathbf{I} & \mathbf{O} \\
-\mathbf{D}^{-1}\mathbf{C} & \mathbf{I}
\end{pmatrix}
=
\begin{pmatrix}
\mathbf{M}/\mathbf{D} & \mathbf{O} \\
\mathbf{O} & \mathbf{D}
\end{pmatrix}
\label{eq:block_diagonalization_of_partitioned_matrix_with_Schur_complement_wrt_D}
\end{equation}
if $\mathbf{A}$ or $\mathbf{D}$ is nonsingular, respectively,
where we have written the Schur complement of $\mathbf{M}$ with respect to
$\mathbf{A}$ or $\mathbf{D}$ as
\begin{equation}
\mathbf{M}/\mathbf{A} \equiv \mathbf{D} - \mathbf{C}\mathbf{A}^{-1}\mathbf{B}
\label{eq:Schur_complement_of_M_wrt_A}
\end{equation}
or
\begin{equation}
\mathbf{M}/\mathbf{D} \equiv \mathbf{A} - \mathbf{B}\mathbf{D}^{-1}\mathbf{C}
\label{eq:Schur_complement_of_M_wrt_D}
\end{equation}
respectively.\footnote{\
Note that the notation for the Schur complement is chosen to suggest that
it has a flavor of division~\citep{Minka:OldNewMatrixAlgebra}.
In fact, taking the determinant on both sides of
\eqref{eq:block_diagonalization_of_partitioned_matrix_with_Schur_complement_wrt_A} and
\eqref{eq:block_diagonalization_of_partitioned_matrix_with_Schur_complement_wrt_D},
we have from the definition of the determinant~(C.10) that
\begin{align}
\operatorname{det}(\mathbf{M}) &=
\operatorname{det}(\mathbf{A}) \operatorname{det}(\mathbf{M}/\mathbf{A}) \\
\operatorname{det}(\mathbf{M}) &=
\operatorname{det}(\mathbf{D}) \operatorname{det}(\mathbf{M}/\mathbf{D}) .
\end{align}}
The above block diagonalization
identities~\eqref{eq:block_diagonalization_of_partitioned_matrix_with_Schur_complement_wrt_A}
and \eqref{eq:block_diagonalization_of_partitioned_matrix_with_Schur_complement_wrt_D} yield
two versions of the inverse partitioned matrix~$\mathbf{M}^{-1}$, i.e.,
\begin{align}
\begin{pmatrix}
\mathbf{A} & \mathbf{B} \\
\mathbf{C} & \mathbf{D}
\end{pmatrix}^{-1}
&=
\begin{pmatrix}
\mathbf{I} & -\mathbf{A}^{-1}\mathbf{B} \\
\mathbf{O} & \mathbf{I}
\end{pmatrix}
\begin{pmatrix}
\mathbf{A}^{-1} & \mathbf{O} \\
\mathbf{O} & \left(\mathbf{M}/\mathbf{A}\right)^{-1}
\end{pmatrix}
\begin{pmatrix}
\mathbf{I} & \mathbf{O} \\
-\mathbf{C}\mathbf{A}^{-1} & \mathbf{I}
\end{pmatrix} \\
&=
\begin{pmatrix}
\mathbf{A}^{-1} +
\mathbf{A}^{-1}\mathbf{B} \left(\mathbf{M}/\mathbf{A}\right)^{-1} \mathbf{C}\mathbf{A}^{-1} &
-\mathbf{A}^{-1}\mathbf{B} \left(\mathbf{M}/\mathbf{A}\right)^{-1} \\
-\left(\mathbf{M}/\mathbf{A}\right)^{-1} \mathbf{C}\mathbf{A}^{-1} &
\left(\mathbf{M}/\mathbf{A}\right)^{-1}
\end{pmatrix}
\end{align}
and
\begin{align}
\begin{pmatrix}
\mathbf{A} & \mathbf{B} \\
\mathbf{C} & \mathbf{D}
\end{pmatrix}^{-1}
&=
\begin{pmatrix}
\mathbf{I} & \mathbf{O} \\
-\mathbf{D}^{-1}\mathbf{C} & \mathbf{I}
\end{pmatrix}
\begin{pmatrix}
\left(\mathbf{M}/\mathbf{D}\right)^{-1} & \mathbf{O} \\
\mathbf{O} & \mathbf{D}^{-1}
\end{pmatrix}
\begin{pmatrix}
\mathbf{I} & -\mathbf{B}\mathbf{D}^{-1} \\
\mathbf{O} & \mathbf{I}
\end{pmatrix} \\
&=
\begin{pmatrix}
\left(\mathbf{M}/\mathbf{D}\right)^{-1} &
-\left(\mathbf{M}/\mathbf{D}\right)^{-1} \mathbf{B}\mathbf{D}^{-1} \\
-\mathbf{D}^{-1}\mathbf{C} \left(\mathbf{M}/\mathbf{D}\right)^{-1} &
\mathbf{D}^{-1} +
\mathbf{D}^{-1}\mathbf{C} \left(\mathbf{M}/\mathbf{D}\right)^{-1} \mathbf{B}\mathbf{D}^{-1}
\end{pmatrix}
\end{align}
respectively.
Equating the right hand sides, we have, e.g.,
\begin{equation}
\left(\mathbf{M}/\mathbf{D}\right)^{-1} =
\mathbf{A}^{-1} +
\mathbf{A}^{-1}\mathbf{B} \left(\mathbf{M}/\mathbf{A}\right)^{-1} \mathbf{C}\mathbf{A}^{-1}
\end{equation}
and
\begin{equation}
-\left(\mathbf{M}/\mathbf{A}\right)^{-1} \mathbf{C}\mathbf{A}^{-1} =
-\mathbf{D}^{-1}\mathbf{C} \left(\mathbf{M}/\mathbf{D}\right)^{-1} .
\end{equation}
Substituting \eqref{eq:Schur_complement_of_M_wrt_A} and \eqref{eq:Schur_complement_of_M_wrt_D}
into both sides and
replacing $\mathbf{D}$ with $-\mathbf{D}$,
we finally have
\begin{equation}
\left(\mathbf{A} + \mathbf{B}\mathbf{D}^{-1}\mathbf{C}\right)^{-1} =
\mathbf{A}^{-1} -
\mathbf{A}^{-1}\mathbf{B}
\left(\mathbf{D} + \mathbf{C}\mathbf{A}^{-1}\mathbf{B}\right)^{-1}
\mathbf{C}\mathbf{A}^{-1}
\end{equation}
and
\begin{equation}
\left(\mathbf{D} + \mathbf{C}\mathbf{A}^{-1}\mathbf{B}\right)^{-1} \mathbf{C}\mathbf{A}^{-1} =
\mathbf{D}^{-1}\mathbf{C} \left(\mathbf{A} + \mathbf{B}\mathbf{D}^{-1}\mathbf{C}\right)^{-1}
\end{equation}
which are equivalent to (C.7) and \eqref{eq:general_push_through_identity}, respectively.

\erratum{Page~697}
Equation~(C.17):
It is clear that the definition~(C.17) of the derivative of a scalar with respect to a vector
contradicts (C.16) and (C.18).
The vector derivative of the form~(C.17) is called the \emph{gradient}
whereas (C.18) is called the \emph{Jacobian}~\citep{Minka:OldNewMatrixAlgebra}.
We should use a different notation, say, $\nabla$ for the gradient to avoid ambiguity.
More specifically, given a vector function~$\mathbf{y}(\mathbf{x}) =
\left(y_1(\mathbf{x}), \dots, y_M(\mathbf{x})\right)^{\operatorname{T}}$
where $\mathbf{x} = \left(x_1, \dots, x_D \right)^{\operatorname{T}}$,
we write the gradient of $\mathbf{y}(\mathbf{x})$ with respect to $\mathbf{x}$ as
\begin{equation}
\nabla_{\mathbf{x}} \mathbf{y} = \left( \frac{\partial y_j}{\partial x_i} \right) =
\begin{pmatrix}
\frac{\partial y_1}{\partial x_1} & \hdots & \frac{\partial y_M}{\partial x_1} \\
\vdots & \ddots & \vdots \\
\frac{\partial y_1}{\partial x_D} & \hdots & \frac{\partial y_M}{\partial x_D}
\end{pmatrix} .
\label{eq:gradient_of_vector_wrt_vector}
\end{equation}
As a special case, we see that the gradient of a scalar function~$y(\mathbf{x})$ with respect to
a column vector~$\mathbf{x}$ is again a column vector of the same dimension,
corresponding to the right hand side of (C.17), i.e.,
\begin{equation}
\nabla_{\mathbf{x}} y = \left( \frac{\partial y}{\partial x_i} \right) =
\begin{pmatrix}
\frac{\partial y}{\partial x_1} \\
\vdots  \\
\frac{\partial y}{\partial x_D}
\end{pmatrix} .
\end{equation}
Note also that the right hand side of the definition of
the gradient~\eqref{eq:gradient_of_vector_wrt_vector} is identical to
the transpose of the Jacobian matrix~$\partial\mathbf{y}/\partial\mathbf{x} =
\left( \partial y_i/\partial x_j \right)$
so that $\nabla_{\mathbf{x}} \mathbf{y} =
\left( \partial\mathbf{y}/\partial\mathbf{x} \right)^{\operatorname{T}}$,
as a consequence of which the chain rule for the gradient is such that the intermediate gradients
are built up ``towards the left,'' i.e.,
\begin{equation}
\nabla_{\mathbf{x}} \mathbf{z}(\mathbf{y}) =
\left(
  \frac{\partial\mathbf{z}}{\partial\mathbf{y}}
  \frac{\partial\mathbf{y}}{\partial\mathbf{x}}
\right)^{\operatorname{T}} =
\nabla_\mathbf{x} \mathbf{y} \nabla_\mathbf{y} \mathbf{z}.
\label{eq:chain_rule_for_gradient}
\end{equation}
Since the chain rule~\eqref{eq:chain_rule_for_gradient} is handy
when we compute the gradients of composite functions,
I would suggest that it should also be pointed out in
``(Vector and) Matrix Derivatives'' of Appendix~C.\footnote{At this point,
one might wonder why we use two different forms of vector derivative that are identical
up to the transposed layout, i.e.,
the gradient~$\nabla_{\mathbf{x}} \mathbf{y}$ and
the Jacobian~$\partial\mathbf{y}/\partial\mathbf{x}$.
As \citet{Minka:OldNewMatrixAlgebra} points out,
Jacobians are useful in calculus while gradients are useful in optimization.}

\erratum{Page~697}
Equation~(C.19):
Following the gradient notation~\eqref{eq:gradient_of_vector_wrt_vector}, (C.19) should read
\begin{equation}
\nabla \left\{ \mathbf{x}^{\operatorname{T}} \mathbf{a} \right\} = 
\nabla \left\{ \mathbf{a}^{\operatorname{T}} \mathbf{x} \right\} = 
\mathbf{a}
\end{equation}
where we have omitted the subscript $\mathbf{x}$ in what should be $\nabla_{\mathbf{x}}$.
Some other useful identities I would suggest to include are
\begin{align}
\nabla \left\{ \mathbf{x}^{\operatorname{T}} \mathbf{A} \mathbf{x} \right\} =
\nabla \operatorname{Tr}\left(\mathbf{x}\mathbf{x}^{\operatorname{T}}\mathbf{A}\right) &=
\left( \mathbf{A}^{\operatorname{T}} + \mathbf{A} \right)\mathbf{x}
\label{eq:gradient_of_quadratic_form} \\
\nabla \left\{ \mathbf{B}\mathbf{x} \right\} & = \mathbf{B}^{\operatorname{T}}
\label{eq:gradient_of_matrix_vector_product} \\
\nabla \left\{ \varphi\mathbf{y} \right\} &=
\nabla\varphi \mathbf{y}^{\operatorname{T}} + \varphi \nabla\mathbf{y}
\label{eq:gradient_of_scalar_vector_product}
\end{align}
where matrices $\mathbf{A}$ and $\mathbf{B}$ are constants.
Note that $\mathbf{x}^{\operatorname{T}} \mathbf{A} \mathbf{x}$ in
\eqref{eq:gradient_of_quadratic_form} is a quadratic form and thus
the square matrix $\mathbf{A}$ is usually taken to be symmetric so that
$\mathbf{A} = \mathbf{A}^{\operatorname{T}}$, in which case we have
\begin{equation}
\nabla \left\{ \mathbf{x}^{\operatorname{T}} \mathbf{A} \mathbf{x} \right\} =
2\mathbf{A} \mathbf{x} \label{eq:gradient_of_quadratic_form_symmetric} .
\end{equation}
Substituting $\mathbf{A} = \mathbf{I}$ gives
\begin{equation}
\nabla \left\| \mathbf{x} \right\|^{2} = 2\mathbf{x} \label{eq:gradiet_of_norm}
\end{equation}
where $\left\| \mathbf{x} \right\| = \sqrt{\mathbf{x}^{\operatorname{T}}\mathbf{x}}$ is
the norm of $\mathbf{x}$.
We make use of the above identity~\eqref{eq:gradiet_of_norm} when, e.g., we take the gradient of
a sum-of-squares error function of the form (3.12),
which can be expressed in terms of the design matrix~$\bm{\Phi}$ given by (3.16) as
\begin{equation}
E(\mathbf{w}) = \frac{1}{2} \left\| \mathbf{t} - \bm{\Phi}\mathbf{w} \right\|^2
\label{eq:sum_of_squares_error} .
\end{equation}
Taking the gradient of \eqref{eq:sum_of_squares_error} with respect to $\mathbf{w}$, we have
\begin{equation}
\nabla_{\mathbf{w}} E(\mathbf{w}) =
-\bm{\Phi}^{\operatorname{T}} \left( \mathbf{t} - \bm{\Phi}\mathbf{w} \right)
\end{equation}
where we have used the identity~\eqref{eq:gradiet_of_norm} together with
the chain rule~\eqref{eq:chain_rule_for_gradient} and
the identity~\eqref{eq:gradient_of_matrix_vector_product}.
The same result can also be obtained by first expanding the square norm in
\eqref{eq:sum_of_squares_error} and then differentiating it using the gradient identities
given above.
We use the identity~\eqref{eq:gradient_of_scalar_vector_product}
when, e.g., we evaluate the Hessian~(5.83) of a nonlinear sum-of-squares error function
such as (5.82), which takes the form
\begin{equation}
J = \frac{1}{2} \sum_{n} \varepsilon_n^{2}.
\end{equation}
The gradient and the Hessian of $J$ can be evaluated as
\begin{align}
\nabla J &= \sum_{n} \varepsilon_n \nabla \varepsilon_n \\
\nabla\nabla J &= \sum_{n} \nabla\varepsilon_n \left(\nabla\varepsilon_n\right)^{\operatorname{T}}
+ \sum_{n} \varepsilon_n \nabla\nabla \varepsilon_n .
\end{align}
It is also worth noting here that we can write down the Taylor series expansion
(up to the second order) of a scalar function $f(\mathbf{x})$ compactly
in terms of the gradients as
\begin{equation}
f(\mathbf{x} + \Delta\mathbf{x}) \simeq
f(\mathbf{x})
+ \mathbf{g}^{\operatorname{T}} \Delta\mathbf{x}
+ \frac{1}{2} {\Delta\mathbf{x}}^{\operatorname{T}} \mathbf{H} \Delta\mathbf{x}
\end{equation}
where $\mathbf{g}$ and $\mathbf{H}$ are the gradient vector and the Hessian matrix of
$f(\mathbf{x})$, respectively, so that
\begin{equation}
\mathbf{g} \equiv \nabla f(\mathbf{x}) =
\begin{pmatrix}
\frac{\partial f}{\partial x_{1}} \\
\vdots \\
\frac{\partial f}{\partial x_{D}}
\end{pmatrix} , \quad
\mathbf{H} \equiv \nabla\nabla f(\mathbf{x}) =
\begin{pmatrix}
\frac{\partial^{2} f}{\partial x_{1} \partial x_{1}} &
\hdots &
\frac{\partial^{2} f}{\partial x_{1} \partial x_{D}} \\
\vdots & \ddots & \vdots \\
\frac{\partial^{2} f}{\partial x_{D} \partial x_{1}} &
\hdots &
\frac{\partial^{2} f}{\partial x_{D} \partial x_{D}}
\end{pmatrix} .
\end{equation}

\erratum{Page~698}
Equation~(C.20):
Although the Jacobian of a vector with respect to a vector is defined in (C.18),
the Jacobian of a matrix with respect to a scalar has not been defined.
The Jacobian~$\partial\mathbf{A}/\partial x$ of
a matrix~$\mathbf{A} = (A_{ij})$ with respect to a scalar~$x$ is
a matrix with the same dimensionality as $\mathbf{A}$
and is given by
\begin{equation}
\frac{\partial\mathbf{A}}{\partial x} = \left( \frac{\partial A_{ij}}{\partial x} \right)
\label{eq:Jacobian_of_matrix_wrt_scalar}
\end{equation}
which is analogous to (C.18) in that
the partial derivatives are laid out according to the numerator, i.e.,
$\mathbf{A}$ in \eqref{eq:Jacobian_of_matrix_wrt_scalar}.
On the other hand, the gradient~\eqref{eq:gradient_of_vector_wrt_vector} is such that
the derivatives are laid out according to the denominator.
We should also point out that,
in a similar analogy, we write the gradient~$\nabla_{\mathbf{A}} y$ of
a scalar~$y$ with respect to a matrix~$\mathbf{A}$ as
\begin{equation}
\nabla_{\mathbf{A}} y = \left( \frac{\partial y}{\partial A_{ij}} \right) .
\label{eq:gradient_of_scalar_wrt_matrix}
\end{equation}

\erratum{Page~698}
Equation~(C.22):
For this identity to be well-defined,
it is necessary that we have $\operatorname{det}(\mathbf{A}) > 0$.
We should make this assumption clear.
Or, if we adopt the notation~\eqref{eq:abs_of_matrix_determinant} for $\left|\mathbf{A}\right|$,
which I would recommend,
we see that (C.22) holds for any nonsingular $\mathbf{A}$ such that
$\operatorname{det}(\mathbf{A}) \neq 0$.
The section named ``Eigenvector Equation'' of Appendix~C gives us a hint for a proof of (C.22)
where $\mathbf{A}$ is assumed to be symmetric positive definite so that $\mathbf{A} \succ 0$.
Although the restricted proof outlined in PRML is indeed highly instructive,
we need a more general proof because we make use of this identity, e.g., in Exercise~2.34
without the assumptions required by the restricted proof.
To this end, we first show the following identity for any square matrix~$\mathbf{A}$
\begin{equation}
\frac{\partial}{\partial x} \operatorname{det}\left(\mathbf{A}\right) =
\operatorname{Tr}\left(\mathbf{A}^{\dagger}\frac{\partial\mathbf{A}}{\partial x}\right)
\label{eq:Jacobi_formula}
\end{equation}
where $\mathbf{A}^{\dagger}$ is the adjugate matrix of
$\mathbf{A}$.
The $(ij)$-th element $A_{ij}^{\dagger}$ of the adjugate matrix~$\mathbf{A}^{\dagger}$ is given by
\begin{equation}
A_{ij}^{\dagger} = (-1)^{i+j} \operatorname{det}\left(\mathbf{A}^{(ji)}\right)
\label{eq:definition_of_adjugate}
\end{equation}
where $\mathbf{A}^{(ij)}$ is a matrix obtained by removing
the $i$-th row and the $j$-th column of $\mathbf{A}$.
From the identity
\begin{equation}
\mathbf{A}\mathbf{A}^{\dagger} = \mathbf{A}^{\dagger}\mathbf{A} =
\operatorname{det}(\mathbf{A})\mathbf{I}
\label{eq:adjugate_identity}
\end{equation}
we can write the inverse matrix~$\mathbf{A}^{-1}$ in terms of
the adjugate matrix~$\mathbf{A}^{\dagger}$ so that
\begin{equation}
\mathbf{A}^{-1} = \frac{\mathbf{A}^{\dagger}}{\operatorname{det}(\mathbf{A})}
\label{eq:inverse_in_terms_of_adjugate}
\end{equation}
if $\mathbf{A}$ is nonsingular so that $\operatorname{det}(\mathbf{A}) \neq 0$.
Note also that the above identity~\eqref{eq:adjugate_identity} implies
\begin{equation}
\operatorname{det}(\mathbf{A}) =
\sum_{k} A_{ik} A_{ki}^{\dagger} = \sum_{k} A_{jk}^{\dagger} A_{kj}
\label{eq:determinant_expansion}
\end{equation}
for any $i$ and $j$.
Substituting this identity~\eqref{eq:determinant_expansion} into
the left hand side of \eqref{eq:Jacobi_formula}
and noting that, from the definition~\eqref{eq:definition_of_adjugate} of the adjugate matrix,
$A_{ji}^{\dagger}$ is independent of $A_{ik}$ nor $A_{kj}$ for any $k$,
we have
\begin{align}
\frac{\partial}{\partial x} \operatorname{det}\left(\mathbf{A}\right) &=
\sum_{ij} \left\{ \frac{\partial}{\partial A_{ij}} \sum_{k} A_{ik} A_{ki}^{\dagger} \right\}
  \frac{\partial A_{ij}}{\partial x} =
\sum_{ij} \left\{ \frac{\partial}{\partial A_{ij}} \sum_{k} A_{jk}^{\dagger} A_{kj} \right\}
  \frac{\partial A_{ij}}{\partial x} \\
&= \sum_{ij} A_{ji}^{\dagger} \frac{\partial A_{ij}}{\partial x} \\
&= 
\operatorname{Tr}\left(\frac{\partial\mathbf{A}}{\partial x}\mathbf{A}^{\dagger}\right) =
\operatorname{Tr}\left(\mathbf{A}^{\dagger}\frac{\partial\mathbf{A}}{\partial x}\right)
\end{align}
which proves the identity~\eqref{eq:Jacobi_formula}.
Making use of \eqref{eq:Jacobi_formula}
together with \eqref{eq:inverse_in_terms_of_adjugate},
we can now evaluate the right hand side of (C.22) as
\begin{align}
\frac{\partial}{\partial x} \ln \left|\mathbf{A}\right| &=
\frac{1}{\operatorname{det}\left(\mathbf{A}\right)}
\frac{\partial}{\partial x} \operatorname{det}\left(\mathbf{A}\right) \\
&=
\operatorname{Tr}\left(\mathbf{A}^{-1}\frac{\partial\mathbf{A}}{\partial x}\right)
\end{align}
if $\mathbf{A}$ is nonsingular so that $\operatorname{det}(\mathbf{A}) \neq 0$
where we have used the notation~\eqref{eq:abs_of_matrix_determinant} for $\left|\mathbf{A}\right|$.

\erratum{Page~698}
Equations~(C.24), (C.25), (C.26), (C.27), and (C.28):
Since these derivatives are gradients, the operator~$\frac{\partial}{\partial \mathbf{A}}$ should
read $\nabla_{\mathbf{A}}$ if we adopt the notation~\eqref{eq:gradient_of_scalar_wrt_matrix}
for the gradient of a scalar with respect to a matrix.
For example, (C.28) should read
\begin{equation}
\nabla_{\mathbf{A}} \ln \left| \mathbf{A} \right| = \mathbf{A}^{-\operatorname{T}}
\end{equation}
where we have used (C.4) and written
\begin{equation}
\left( \mathbf{A}^{-1}\right )^{\operatorname{T}} =
\left( \mathbf{A}^{\operatorname{T}}\right )^{-1} =
\mathbf{A}^{-\operatorname{T}} .
\end{equation}
In addition to these identities regarding gradients with respect to a matrix,
I would suggest to include two more identities
\begin{align}
\nabla_{\mathbf{A}} \operatorname{Tr}\left(
\mathbf{A}\mathbf{B}\mathbf{A}^{\operatorname{T}}\mathbf{C} \right) &=
\mathbf{C}^{\operatorname{T}} \mathbf{A} \mathbf{B}^{\operatorname{T}} +
\mathbf{C} \mathbf{A} \mathbf{B}
\label{eq:gradient_of_Tr_A_B_Atranspose_C} \\
\nabla_{\mathbf{A}} \operatorname{Tr}\left(\mathbf{A}^{-1}\mathbf{B}\right) &=
- \mathbf{A}^{-\operatorname{T}} \mathbf{B}^{\operatorname{T}} \mathbf{A}^{-\operatorname{T}}.
\label{eq:gradient_of_Tr_Ainverse_B}
\end{align}
We use the above identities~\eqref{eq:gradient_of_Tr_A_B_Atranspose_C} and
\eqref{eq:gradient_of_Tr_Ainverse_B}, e.g.,
when we show (13.113) in Exercise~13.33 and (2.122) in Exercise~2.34, respectively.
It should also be noted that (C.27) is a special case of
the former identity~\eqref{eq:gradient_of_Tr_A_B_Atranspose_C}.

\erratum{Page~700}
The second paragraph, Line~$-1$:
The determinant of $\mathbf{U}$ can be either positive or negative so that
we should write $\operatorname{det}(\mathbf{U}) = \pm 1$, which is equivalent to
$\left|\mathbf{U}\right| = 1$ if we adopt the notation~\eqref{eq:abs_of_matrix_determinant}.
Although it is possible to take $\mathbf{U}$ such that $\operatorname{det}(\mathbf{U}) = 1$,
there is no point in doing so in practice.
In a special case of $\mathbf{A}$ being positive semidefinite or $\mathbf{A} \succeq 0$,
we can identify the eigenvalue decomposition~(C.43) with
the \emph{singular value decomposition}~\citep{Press:NR,GvL:MC}
\begin{equation}
\mathbf{A} = \mathbf{U}\bm{\Sigma}\mathbf{V}^{\operatorname{T}}
\end{equation}
where $\mathbf{U} = \mathbf{V}$ and
the eigenvalues~$\lambda_i$ with the singular values~$\sigma_i \geq 0$.
The singular values are usually arranged in descending order in
the diagonal matrix~$\bm{\Sigma} = \operatorname{diag}\left(\sigma_1, \dots, \sigma_M\right)$
so that $\sigma_1 \geq \dots \geq \sigma_M \geq 0$,
in which case it is no longer possible in general to take the orthonormal matrix~$\mathbf{U}$
such that $\operatorname{det}(\mathbf{U}) = 1$.

\erratum{Page~700}
The text following (C.41):
The multiplication by $\mathbf{U}$ can be interpreted as a rotation, a reflection,
or a combination of the two.

\erratum{Page~705}
Equation~(D.8):
It would be helpful if we make it clear that the left hand side of (D.8) corresponds to
the functional derivative so that we modify (D.8) as
\begin{equation}
\frac{\delta F}{\delta y(x)} =
\frac{\partial G}{\partial y} -
\frac{\mathrm{d}}{\mathrm{d}x} \left( \frac{\partial G}{\partial y'} \right) = 0 .
\end{equation}

\erratum{Page~705}
The last paragraph:
Despite the statement,
it is not quite straightforward to extend the results obtained here to higher dimensions.
Although such an extension is not required in PRML, it can be used to analyze
a particular type of constrained optimization problem commonly found in
computer vision applications such as \emph{optical flow}~\citep{HornSchunck:OpticalFlow}.
Here, I would like to consider an extension of the calculus of variations to
a system of $D$-dimensional Cartesian coordinates~$\mathbf{x} =
\left( x_1, \dots, x_D \right)^{\operatorname{T}} \in \mathbb{R}^{D}$
and find a form of the functional derivative as well as a more general boundary condition
for the derivative to be well-defined.
To this end, we first review some identities concerning
the \emph{divergence}~\citep{Feynman:FeynmanLectures2}.
The divergence of a vector field~$\mathbf{p}(\mathbf{x}) =
\left( p_1(\mathbf{x}), \dots, p_D(\mathbf{x}) \right)^{\operatorname{T}} \in \mathbb{R}^{D}$
is a scalar field of the form
\begin{equation}
\operatorname{div} \mathbf{p} = \sum_{i} \frac{\partial p_i}{\partial x_i} \equiv
\nabla \cdot \mathbf{p}
\end{equation}
where we have omitted the coordinates~$\mathbf{x}$ in the function arguments
to keep the notation uncluttered.
For a differentiable vector field~$\mathbf{p}(\mathbf{x})$ defined on
some volume~$\Omega \subset \mathbb{R}^D$,
the \emph{divergence theorem}~\citep{Feynman:FeynmanLectures2} holds so that
\begin{equation}
\int_{\Omega} \operatorname{div} \mathbf{p} \; \mathrm{d}V =
\oint_{\partial\Omega} \mathbf{p} \cdot \mathbf{n} \; \mathrm{d}S
\label{eq:divergence_theorem}
\end{equation}
where the left hand side is the volume integral over the volume~$\Omega$;
the right hand side is the surface integral over its boundary~$\partial\Omega$; and
$\mathbf{n}(\mathbf{x})$ is the outward unit normal vector of $\partial\Omega$.
Assuming that $\mathbf{x}$ is a system of Cartesian coordinates,
we can write the volume element as
$\mathrm{d}V = \mathrm{d}x_1 \cdots \mathrm{d}x_D \equiv \mathrm{d}\mathbf{x}$ and
the inner product as $\mathbf{p} \cdot \mathbf{n} = \mathbf{p}^{\operatorname{T}} \mathbf{n}$.
Making use of the divergence theorem~\eqref{eq:divergence_theorem} together with the identity
\begin{equation}
\operatorname{div}\left(\varphi\mathbf{p}\right) =
{\nabla\varphi}^{\operatorname{T}} \mathbf{p} + \varphi \operatorname{div}\mathbf{p}
\end{equation}
we obtain a multidimensional version of the ``integration by parts'' formula
\begin{equation}
\int_{\Omega} {\nabla\varphi}^{\operatorname{T}} \mathbf{p} \; \mathrm{d}\mathbf{x} =
\oint_{\partial\Omega} \varphi\mathbf{p}^{\operatorname{T}} \mathbf{n} \; \mathrm{d}S -
\int_{\Omega} \varphi \operatorname{div}\mathbf{p} \; \mathrm{d}\mathbf{x} .
\label{eq:multidimensional_integration_by_parts}
\end{equation}
Let us now consider a functional of the form
\begin{equation}
E\left[u(\mathbf{x})\right] =
\int_{\Omega} L\left(\mathbf{x}, u(\mathbf{x}), \nabla u(\mathbf{x})\right) \mathrm{d}\mathbf{x}
\end{equation}
where $u(\mathbf{x}) \in \mathbb{R}$ is a function (scalar field) defined over
some volume~$\Omega \subset \mathbb{R}^{D}$ and
$L(\mathbf{x}, f, \mathbf{g}) \in \mathbb{R}$ is a function of
$\mathbf{x} \in \Omega$, $f \in \mathbb{R}$, and $\mathbf{g} \in \mathbb{R}^{D}$.
Thus, the functional~$E\left[u(\mathbf{x})\right] \in \mathbb{R}$ maps $u(\mathbf{x})$ to
some real number.
As in the ordinary calculus, we can define the derivative of a functional according to
the \emph{calculus of variations}~\citep{Feynman:FeynmanLectures2,Bishop:PRML}.
In order to find the form of the functional derivative,
we consider how $E\left[u(\mathbf{x})\right]$ varies upon a small change~$\epsilon\eta(\mathbf{x})$
to $u(\mathbf{x})$
where $\eta(\mathbf{x})$ is the ``direction'' of a change we try to make in $u(\mathbf{x})$ and
$\epsilon$ is a small constant.
The first-order variation of $E\left[u(\mathbf{x})\right]$ in the direction of $\eta(\mathbf{x})$
can be evaluated as
\begin{align}
\delta E[u; \eta] &\equiv
\lim_{\epsilon \to 0} \frac{1}{\epsilon} \left\{ E[u + \epsilon\eta] - E[u] \right\} \\
&= \lim_{\epsilon \to 0} \frac{1}{\epsilon}
\int_{\Omega} \left\{
L\left(\mathbf{x}, u + \epsilon\eta, \nabla (u + \epsilon\eta)\right) -
L\left(\mathbf{x}, u, \nabla u\right)
\right\} \mathrm{d}\mathbf{x} \\
&= \int_{\Omega} \left\{
\eta\frac{\partial L}{\partial f} +
{\nabla\eta}^{\operatorname{T}} \nabla_{\mathbf{g}} L
\right\} \mathrm{d}\mathbf{x}
\label{eq:first_order_variation_of_E_wrt_u_in_eta_before_expansion}
\end{align}
where we have assumed that $L(\mathbf{x}, f, \mathbf{g})$ is differentiable with respect to
both $f$ and $\mathbf{g}$; and we have written
\begin{equation}
\frac{\partial L}{\partial f} \equiv
\frac{\partial}{\partial f} L\left(\mathbf{x}, u, \nabla u\right) , \qquad
\nabla_{\mathbf{g}} L \equiv 
\nabla_{\mathbf{g}} L\left(\mathbf{x}, u, \nabla u\right) .
\end{equation}
Expanding the second term in \eqref{eq:first_order_variation_of_E_wrt_u_in_eta_before_expansion}
with the multidimensional integration by parts~\eqref{eq:multidimensional_integration_by_parts},
we have
\begin{equation}
\delta E[u; \eta] =
\int_{\Omega} \eta \left\{
\frac{\partial L}{ \partial f} - \operatorname{div}\left( \nabla_{\mathbf{g}} L \right)
\right\} \mathrm{d}\mathbf{x} +
\oint_{\partial\Omega} \eta {\nabla_{\mathbf{g}} L}^{\operatorname{T}} \mathbf{n} \; \mathrm{d}S .
\label{eq:first_order_variation_of_E_wrt_u_in_eta}
\end{equation}
In order for the functional derivative to be well-defined,
we assume the second term in
the variation~\eqref{eq:first_order_variation_of_E_wrt_u_in_eta} to vanish so that
\begin{equation}
\oint_{\partial\Omega} \eta {\nabla_{\mathbf{g}} L}^{\operatorname{T}} \mathbf{n} \; \mathrm{d}S
= 0.
\label{eq:boundary_condition_for_multidimensional_calculus_of_variations}
\end{equation}
We shall examine this boundary condition later in more detail.
If we write
\begin{equation}
\frac{\partial E}{\partial u(\mathbf{x})} \equiv
\frac{\partial L}{ \partial f} - \operatorname{div}\left( \nabla_{\mathbf{g}} L \right)
\end{equation}
the first term in the variation~\eqref{eq:first_order_variation_of_E_wrt_u_in_eta} can be seen as
the inner product of $\eta(\mathbf{x})$ and $\partial E/\partial u(\mathbf{x})$, from which
we conclude that the quantity~$\partial E/\partial u(\mathbf{x})$ is what should be called
the functional derivative.
A stationary point~$u(\mathbf{x})$ of the functional~$E[u(\mathbf{x})]$,
at which the variation~$\delta E[u; \eta]$ vanishes in any given direction~$\eta(\mathbf{x})$,
satisfies the \emph{Eular-Lagrange equation} given by
\begin{equation}
\frac{\partial E}{\partial u(\mathbf{x})} = 0 .
\end{equation}
The boundary condition~\eqref{eq:boundary_condition_for_multidimensional_calculus_of_variations}
holds if
\begin{equation}
\eta(\mathbf{x}) = 0
\label{eq:fixed_boundary_condition_for_calculus_of_variations}
\end{equation}
or 
\begin{equation}
{\nabla_{\mathbf{g}} L}^{\operatorname{T}} \mathbf{n} = 0
\label{eq:derivative_boundary_condition_for_calculus_of_variations}
\end{equation}
where $\mathbf{x} \in \partial\Omega$.
The first condition~\eqref{eq:fixed_boundary_condition_for_calculus_of_variations} holds
if we assume the \emph{Dirichlet boundary condition} for $u(\mathbf{x})$
\begin{equation}
u(\mathbf{x}) = u_0(\mathbf{x})
\label{eq:Dirichlet_boundary_condition}
\end{equation}
where $\mathbf{x} \in \partial\Omega$, i.e.,
$u(\mathbf{x})$ is assumed to be fixed to some value~$u_0(\mathbf{x})$
at the boundary~$\partial\Omega$ and
so is $u(\mathbf{x}) + \epsilon\eta(\mathbf{x})$,
implying \eqref{eq:fixed_boundary_condition_for_calculus_of_variations}.
Another common boundary condition for $u(\mathbf{x})$ is the \emph{Neumann boundary condition}
\begin{equation}
{\nabla u}^{\operatorname{T}} \mathbf{n} = 0
\label{eq:Neumann_boundary_condition}
\end{equation}
where $\mathbf{x} \in \partial\Omega$.
The Neumann boundary condition~\eqref{eq:Neumann_boundary_condition} implies
the second condition~\eqref{eq:derivative_boundary_condition_for_calculus_of_variations}
for a broad type of optical-flow energy functional.
TODO: Add an example of the Horn-Schunck optical-flow energy functional.

\erratum{Page~716}
Column~1, Entry~$-1$:
``The Feynman Lectures of Physics'' should read ``The Feynman Lectures on Physics.''

\erratum{Page~717}
Column~2, Entry~7:
``John Hopkins University Press'' should read ``The Johns Hopkins University Press.''

\bibliographystyle{chicago}
\bibliography{refs}

\end{document}
